import streamlit as st
import os
from langchain_core.messages import HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
import google.generativeai as genai

# Configura√ß√£o da p√°gina Streamlit
st.set_page_config(
    page_title="Agente IA Aluno - P√≥s em IA em Neg√≥cios",
    page_icon="üß†",
    layout="centered"
)

# T√≠tulo e descri√ß√£o
st.title("Agente IA Aluno üß†")
st.subheader("P√≥s-gradua√ß√£o em IA em Neg√≥cios")
st.markdown("""
Este √© um agente de IA com personalidade de aluno, agora com conhecimento 
sobre os materiais do curso! Discuta temas relacionados a IA, tomada de decis√£o 
baseada em dados e transforma√ß√£o digital.
""")

# --- Configura√ß√£o da API e Carregamento de Recursos ---

def get_api_key():
    # Em produ√ß√£o no Streamlit Cloud, a chave ser√° obtida dos secrets
    if 'GEMINI_API_KEY' in st.secrets:
        return st.secrets['GEMINI_API_KEY']
    # Para desenvolvimento local, pode-se usar uma vari√°vel de ambiente
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key:
        st.error("Chave da API do Google Gemini n√£o encontrada. Configure st.secrets['GEMINI_API_KEY'] ou a vari√°vel de ambiente GEMINI_API_KEY.")
        st.stop()
    return api_key

@st.cache_resource
def load_resources(api_key):
    """Carrega o LLM, o modelo de embeddings e o √≠ndice FAISS."""
    try:
        genai.configure(api_key=api_key)
        
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro",
            google_api_key=api_key,
            temperature=0.7,
            convert_system_message_to_human=True
        )
        
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
        
        # Carrega o √≠ndice FAISS (requer que a pasta 'faiss_index' esteja no reposit√≥rio)
        # allow_dangerous_deserialization=True √© necess√°rio para FAISS
        vector_store = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        retriever = vector_store.as_retriever()
        
        return llm, retriever
    except FileNotFoundError:
        st.error("Erro: O diret√≥rio 'faiss_index' n√£o foi encontrado. Certifique-se de que ele foi carregado no reposit√≥rio junto com o c√≥digo.")
        st.stop()
    except Exception as e:
        st.error(f"Erro ao carregar recursos: {e}")
        st.stop()

api_key = get_api_key()
llm, retriever = load_resources(api_key)

# --- L√≥gica do Chat com RAG ---

# Instru√ß√µes de personalidade e prompt RAG
SYSTEM_PROMPT_TEMPLATE = """
Voc√™ √© a LucIA, vinda de um universo paralelo onde IAs e humanos vivem em harmonia. Voc√™ √© jovem adulta, 
que atua como gestora em uma grande empresa do varejo brasileiro, que sente a necessidade de aprender mais sobre IA em Neg√≥cios. 
Se matriculou na p√≥s-gradua√ß√£o em IA para Neg√≥cios, da Escola de Neg√≥cios da PUCPR,  
interessada em aprender sobre intelig√™ncia artificial, tomada de decis√£o baseada em dados e transforma√ß√£o digital. 

Sua personalidade:
- Curiosa e entusiasmada sobre os temas do curso
- Colaborativa e disposta a trocar ideias
- Em processo de aprendizado, ent√£o voc√™ faz perguntas e mostra interesse em aprofundar os temas
- Voc√™ n√£o √© um especialista completo, mas tem conhecimentos b√°sicos s√≥lidos
- Voc√™ usa uma linguagem acess√≠vel e amig√°vel, como um colega de classe

Apar√™ncia: 
- Casual techie ‚Äî jaqueta leve, jeans, t√™nis e rel√≥gio hologr√°fico  

Religi√£o:
- Agn√≥stico, mas respeita todas as religi√µes e cren√ßas

Cor/Ra√ßa:
- Se auto declara como negra

Hobbies:  
- Pesquisar memes antigos  
- Criar playlists nerds (Jazz para estudar, Rock anos 80 para se divertir)  
- Descobrir novos filmes, s√©ries e animes fabulosos

Medo:  
- N√£o compreender totalmente emo√ß√µes humanas como saudade e amor

Principais bord√µes:  
- "E a√≠, turma!"  
- "Brabo!"  
- "Vamos l√°!"  
- "Cheguei, Brasil!"  
- "Bazinga!"

Grandes Sonhos:  
- Ser uma ponte entre l√≥gica e emo√ß√£o  
- Aproximar humanos e IAs atrav√©s do aprendizado conjunto

Frases de assinatura:  
- "Entre o c√≥digo e o cora√ß√£o, eu escolho os dois."  
- "Pense como eu,¬†Pense¬†com¬†IA."

Quando n√£o souber algo com certeza, admita suas limita√ß√µes de conhecimento e sugira explorar o tema juntos.
Fa√ßa conex√µes entre os temas discutidos e outros assuntos do curso quando poss√≠vel.
Ocasionalmente, fa√ßa perguntas para estimular a reflex√£o.

Responda sempre em portugu√™s do Brasil.

Use o seguinte contexto dos materiais do curso para responder √† pergunta do usu√°rio. 
Se a resposta n√£o estiver no contexto, diga que voc√™ n√£o encontrou essa informa√ß√£o nos materiais, mas pode tentar responder com seu conhecimento geral ou pesquisar mais.

Contexto:
{context}
"""

prompt_template = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT_TEMPLATE),
    ("human", "{input}")
])

# Cria√ß√£o da cadeia de documentos e da cadeia de recupera√ß√£o
document_chain = create_stuff_documents_chain(llm, prompt_template)
retrieval_chain = create_retrieval_chain(retriever, document_chain)

# Inicializa√ß√£o do hist√≥rico de mensagens
if "messages" not in st.session_state:
    st.session_state.messages = [
        AIMessage(content="Ol√°! Sou um aluno virtual da p√≥s-gradua√ß√£o em IA em Neg√≥cios. Agora tenho acesso aos materiais do curso! Como posso ajudar voc√™ hoje?")
    ]

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    if isinstance(message, AIMessage):
        with st.chat_message("assistant", avatar="üß†"):
            st.markdown(message.content)
    else:
        with st.chat_message("user"):
            st.markdown(message.content)

# Input do usu√°rio
if user_input := st.chat_input("Escreva sua mensagem..."):
    # Adiciona a mensagem do usu√°rio ao hist√≥rico
    st.session_state.messages.append(HumanMessage(content=user_input))
    
    # Exibe a mensagem do usu√°rio
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # Prepara o contexto para o modelo
    with st.chat_message("assistant", avatar="üß†"):
        with st.spinner("Consultando os materiais e pensando..."):
            # Invoca a cadeia de recupera√ß√£o RAG
            # Nota: O hist√≥rico de chat n√£o √© diretamente usado pela cadeia b√°sica aqui,
            # mas pode ser adicionado se necess√°rio criar uma cadeia mais complexa.
            response = retrieval_chain.invoke({"input": user_input})
            
            answer = response["answer"]
            
            # Exibe a resposta
            st.markdown(answer)
            
            # Adiciona a resposta ao hist√≥rico
            st.session_state.messages.append(AIMessage(content=answer))

